Model Name	Number of Parameters (Billions)	Number of Tokens (Billions)	Number of GPUs	teraFlOPs per GPU	Number of Epochs	Dataset Size (GB)	GPU Count	Training Time (Days)	Data Transfer Rate (GBits/second)
pythia-1b	1.011781632	300	1	75.91	0.89	886	1024	0.36156413296447537	0.20193640990970937
pythia-2.8b	2.77520896	300	1	204.87	0.89	886	1024	0.367464055796901	0.19869416290152075
tiny-llama-1.1b	1.100048384	300	1	86.48	3.0	1410	1024	0.3450592301367047	1.1350708297572483
open_llama_3b	3.4264736	1000	1	245.42	1.0	2734	1024	1.2624486613273693	0.20052153873883632
HyperCLOVA	82.0	150	1	312.0	1.0	2734	1024	3.5647313479344733	0.07101464975609756
NVIDIA	175.0	300	1024	140.0	1.0	2734	1024	33.908420138888886	0.007465642666666667
